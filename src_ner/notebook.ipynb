{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\linhpn.VISC\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import utils_nlp\n",
    "import pickle\n",
    "import ner_model\n",
    "import tensorflow as tf\n",
    "import dataset\n",
    "from dataset import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# dataset_filepaths, dataset_brat_folders = utils.get_valid_dataset_filepaths(parameters)\n",
    "# dataset = ds.DatasetP(verbose=False, debug=False)\n",
    "token_to_vector = utils_nlp.load_pretrained_token_embeddings('../../../ML_EntityData/embedding/en/glove.6B.100d.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab = Dataset(token_to_vector,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens,labels = dataset.parse_conll('../../../ML_EntityData/data/en/train.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab.build_vocabulary(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab.build_labels(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23436"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab.tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "token_indices, character_indices_padded, token_lengths, pattern ,label_indices , label_vector= vocab.transform(tokens,labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedded_characters: Tensor(\"character_embedding/embedded_characters:0\", shape=(?, ?, 25), dtype=float32)\n",
      "embedded_tokens: Tensor(\"token_embedding/embedding_lookup:0\", shape=(?, 100), dtype=float32)\n",
      "token_lstm_input: Tensor(\"concatenate_token_and_character_vectors/token_lstm_input:0\", shape=(?, 163), dtype=float32)\n",
      "token_lstm_input_drop: Tensor(\"dropout/token_lstm_input_drop/mul:0\", shape=(?, 163), dtype=float32)\n",
      "token_lstm_input_drop_expanded: Tensor(\"dropout/token_lstm_input_drop_expanded:0\", shape=(1, ?, 163), dtype=float32)\n",
      "unary_scores_expanded: Tensor(\"crf/unary_scores_expanded:0\", shape=(1, ?, 12), dtype=float32)\n",
      "input_label_indices_flat_batch: Tensor(\"crf/input_label_indices_flat_batch:0\", shape=(1, ?), dtype=int32)\n",
      "sequence_lengths: Tensor(\"crf/sequence_lengths:0\", shape=(1,), dtype=int32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\linhpn.VISC\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_impl.py:98: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    }
   ],
   "source": [
    "model = ner_model.BLSTM_CRF( dataset = vocab, token_embedding_dimension = 100 , character_lstm_hidden_state_dimension = 25,\n",
    "                 token_lstm_hidden_state_dimension = 50 , character_embedding_dimension = 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n"
     ]
    }
   ],
   "source": [
    "sess.run(model.token_embedding_weights.assign(vocab.get_embedding(token_to_vector,100)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy :  1 loss :  20.331879\n",
      "accuracy :  1 loss :  11.125923\n",
      "accuracy :  1 loss :  19.11522\n",
      "accuracy :  1 loss :  7.1867523\n",
      "accuracy :  1 loss :  0.8824196\n",
      "accuracy :  1 loss :  4.8396873\n",
      "accuracy :  1 loss :  6.703743\n",
      "accuracy :  1 loss :  1.5310974\n",
      "accuracy :  1 loss :  1.9914784\n",
      "accuracy :  1 loss :  0.012184143\n",
      "accuracy :  1 loss :  0.478405\n",
      "accuracy :  1 loss :  7.863373\n",
      "accuracy :  1 loss :  0.107922554\n",
      "accuracy :  1 loss :  1.3921509\n",
      "accuracy :  1 loss :  2.2625046\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-8.99765968e-01, -3.37564826e-01, -5.49509168e-01,\n",
       "        -3.61137897e-01, -3.86134833e-01, -2.47585803e-01,\n",
       "         3.25152206e+00, -3.11443418e-01, -1.57752708e-01,\n",
       "         4.29865599e-01, -4.93717074e-01, -4.89027858e-01],\n",
       "       [ 1.70925748e+00,  7.96052754e-01,  1.58780837e+00,\n",
       "         1.89074624e+00, -2.01285172e+00,  1.83266199e+00,\n",
       "        -2.17760420e+00, -1.64479470e+00, -1.23317170e+00,\n",
       "        -3.76378857e-02, -3.52226973e-01,  5.49042106e-01],\n",
       "       [ 5.01621515e-02, -2.04380299e-03,  1.36857167e-01,\n",
       "        -3.78809243e-01, -3.34768891e-02, -2.50578284e-01,\n",
       "        -3.93024534e-01,  2.57517314e+00, -9.19675746e-04,\n",
       "        -3.99849117e-01, -1.75878048e-01, -1.29035739e-02],\n",
       "       [-3.75232369e-01, -4.44905251e-01, -3.26523781e-01,\n",
       "        -9.34358537e-01,  4.01447916e+00, -8.35896671e-01,\n",
       "        -7.66457021e-01, -5.00435472e-01,  4.92477007e-02,\n",
       "         3.32046062e-01, -4.13006306e-01, -4.34453338e-01],\n",
       "       [-1.32810757e-01,  6.60662830e-01,  5.78819923e-02,\n",
       "        -4.83189076e-01,  7.41816998e-01,  1.64729333e-03,\n",
       "         5.62193338e-03, -2.11065546e-01,  2.49215886e-01,\n",
       "         4.27366853e-01,  1.98363781e-01,  5.90703487e-01],\n",
       "       [-5.18597305e-01,  6.50515631e-02, -4.57193583e-01,\n",
       "        -4.64927763e-01, -4.66395199e-01, -6.93442345e-01,\n",
       "        -5.96176028e-01, -5.18076062e-01,  2.87201071e+00,\n",
       "        -1.06263883e-01,  3.74621630e-01, -1.70562401e-01],\n",
       "       [ 3.00306864e-02, -9.14904252e-02,  2.27813706e-01,\n",
       "         1.17332973e-01,  8.57684389e-02, -1.35697991e-01,\n",
       "         2.69906497e+00, -1.69863239e-01, -4.19951290e-01,\n",
       "         1.77499875e-01,  4.48273897e-01, -1.85181722e-01],\n",
       "       [-2.25136504e-01, -3.08065325e-01, -3.39247972e-01,\n",
       "         1.22828698e-02,  2.60707885e-01,  1.50171489e-01,\n",
       "         2.08989546e-01,  1.40070188e+00, -2.03307867e-01,\n",
       "        -3.41651648e-01,  2.93508530e-01, -1.52138054e-01],\n",
       "       [-2.67484993e-01,  3.55930597e-01,  9.73429978e-02,\n",
       "        -4.49939042e-01,  1.96186587e-01,  5.96212111e-02,\n",
       "        -5.16146481e-01, -9.89402980e-02,  3.37562710e-01,\n",
       "         3.74040782e-01,  4.04243350e-01, -1.29290655e-01],\n",
       "       [ 7.78879151e-02, -5.94609201e-01,  2.19565764e-01,\n",
       "         2.03451850e-02,  1.31350935e-01, -3.11956882e-01,\n",
       "        -1.20892569e-01, -3.29901934e-01,  1.02274515e-01,\n",
       "        -2.95130968e-01, -4.37425256e-01, -1.87482521e-01],\n",
       "       [ 8.90604258e-01,  1.98185399e-01,  3.84995997e-01,\n",
       "         5.31295180e-01, -9.51561391e-01,  1.34038997e+00,\n",
       "        -9.26048219e-01, -2.87156194e-01, -8.90140533e-02,\n",
       "         1.20078675e-01, -3.89812469e-01, -3.59030485e-01],\n",
       "       [ 1.60915136e-01,  1.51440382e-01,  3.53800297e-01,\n",
       "        -5.86891174e-02, -2.44968295e-01, -1.61650062e-01,\n",
       "        -4.91476178e-01, -4.19887781e-01,  2.01964259e-01,\n",
       "        -1.86543465e-01,  3.16762924e-02, -3.52589250e-01]], dtype=float32)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.train_step(sess,token_indices, character_indices_padded, token_lengths, pattern ,label_indices , label_vector ,0.5 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../../model/model.ckpt'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.saver.save(sess, '../../model/model.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['B-ORG', 'O', 'B-MISC', 'B-PER', 'I-PER', 'B-LOC', 'I-ORG', 'I-MISC', 'I-LOC']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18416"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab.get_embedding(token_to_vector,100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(text):\n",
    "    text = text+'.'\n",
    "    sentences = vocab.tokenizer(text)\n",
    "    token_indices_test, character_indices_padded_test, token_lengths_test, pattern_test = vocab.transform(sentences)\n",
    "    pre = model.predict(sess,token_indices_test, character_indices_padded_test, token_lengths_test, pattern_test )\n",
    "    \n",
    "    prediction_output =[ [vocab.labels[i] for i in sentence ] for sentence in pre]\n",
    "       \n",
    "    tokens=[]\n",
    "    entitys=[]\n",
    "    for i,sentence in enumerate(prediction_output):\n",
    "        token = ''\n",
    "        previous_label= 'O'\n",
    "        sentence = utils_nlp.bioes_to_bio(sentence)\n",
    "        for j,label in enumerate(sentence):\n",
    "            if label!= 'O':\n",
    "                label = label.split('-')\n",
    "                prefix = label[0]\n",
    "                if prefix == 'B' or previous_label != label[1]:\n",
    "                    if previous_label != 'O':\n",
    "                        tokens.append(token)\n",
    "                        entitys.append(previous_label)\n",
    "                        token = ''\n",
    "                    previous_label = label[1]\n",
    "                    token = sentences[i][j]  #self.dataset.index_to_token[self.dataset.token_indices[dataset_type][i][j]]\n",
    "                else:\n",
    "                    token = token + ' ' + sentences[i][j] # self.dataset.index_to_token[self.dataset.token_indices[dataset_type][i][j]]\n",
    "\n",
    "            else:\n",
    "                if previous_label != 'O':\n",
    "                    tokens.append(token)\n",
    "                    entitys.append(previous_label)\n",
    "                    token=''\n",
    "                previous_label = 'O'    \n",
    "    return list(zip(tokens,entitys))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Ngoc Linh', 'PER'), ('Noi', 'ORG')]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict('my name is Ngoc Linh, I am working at Noi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([[1051, 3228, 446, 8337, 8338, 11]],\n",
       " [[[40, 43, 0, 0],\n",
       "   [31, 34, 40, 28],\n",
       "   [33, 29, 0, 0],\n",
       "   [16, 46, 37, 44],\n",
       "   [8, 33, 31, 39],\n",
       "   [19, 0, 0, 0]]],\n",
       " [[2, 4, 2, 4, 4, 1]],\n",
       " [[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "   [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "   [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickle.dump(vocab.get_embedding(token_to_vector,100),open('../../vocab.pickle','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20971616"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sys.getsizeof(token_to_vector)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
