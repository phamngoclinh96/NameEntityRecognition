{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\linhpn.VISC\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import matplotlib\n",
    "\n",
    "matplotlib.use('Agg')\n",
    "import train_P as train\n",
    "import dataset as ds\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.tensorboard.plugins import projector\n",
    "\n",
    "import utils\n",
    "import os\n",
    "import conll2brat\n",
    "import glob\n",
    "import codecs\n",
    "import shutil\n",
    "import time\n",
    "import copy\n",
    "import evaluate\n",
    "import random\n",
    "import pickle\n",
    "import brat2conll\n",
    "import numpy as np\n",
    "import utils_nlp\n",
    "import distutils.util as distutils_util\n",
    "import configparser\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "parameters = {'pretrained_model_folder':'../model',\n",
    "                      'dataset_text_folder':'../../../ML_EntityData/data/en',\n",
    "                      'character_embedding_dimension':25,\n",
    "                      'character_lstm_hidden_state_dimension':25,\n",
    "                      'check_for_digits_replaced_with_zeros':True,\n",
    "                      'check_for_lowercase':True,\n",
    "                      'debug':False,\n",
    "                      'dropout_rate':0.5,\n",
    "                      'experiment_name':'test',\n",
    "                      'freeze_token_embeddings':False,\n",
    "                      'gradient_clipping_value':5.0,\n",
    "                      'learning_rate':0.005,\n",
    "                      'load_only_pretrained_token_embeddings':False,\n",
    "                      'load_all_pretrained_token_embeddings':False,\n",
    "#                       'main_evaluation_mode':'conll',\n",
    "                      'maximum_number_of_epochs':500,\n",
    "                      'number_of_cpu_threads':8,\n",
    "                      'number_of_gpus':0,\n",
    "                      'optimizer':'sgd',\n",
    "                      'output_folder':'../../../ML_EntityData/output',\n",
    "                      'patience':10,\n",
    "#                       'plot_format':'pdf',\n",
    "                      'reload_character_embeddings':True,\n",
    "                      'reload_character_lstm':True,\n",
    "                      'reload_crf':True,\n",
    "                      'reload_feedforward':True,\n",
    "                      'reload_token_embeddings':True,\n",
    "                      'reload_token_lstm':True,\n",
    "                      'remap_unknown_tokens_to_unk':True,\n",
    "                      'spacylanguage':'en',\n",
    "                      'tagging_format':'bioes',\n",
    "                      'token_embedding_dimension':100,\n",
    "                      'token_lstm_hidden_state_dimension':100,\n",
    "                      'token_pretrained_embedding_filepath':'../../../ML_EntityData/embedding/en/glove.6B.100d.txt',\n",
    "                      'tokenizer':'spacy',\n",
    "                      'train_model':True,\n",
    "                      'use_character_lstm':True,\n",
    "#                       'use_crf':True,\n",
    "                      'use_pretrained_model':False,\n",
    "                      'verbose':False}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking the validity of BRAT-formatted train set... Done.\n",
      "Checking compatibility between CONLL and BRAT for train_compatible_with_brat set ... Done.\n",
      "Checking validity of CONLL BIOES format... Done.\n",
      "Checking the validity of BRAT-formatted valid set... Done.\n",
      "Checking compatibility between CONLL and BRAT for valid_compatible_with_brat set ... Done.\n",
      "Checking validity of CONLL BIOES format... Done.\n",
      "Checking the validity of BRAT-formatted test set... Done.\n",
      "Checking compatibility between CONLL and BRAT for test_compatible_with_brat set ... Done.\n",
      "Checking validity of CONLL BIOES format... Done.\n",
      "Load dataset... done (47.16 seconds)\n"
     ]
    }
   ],
   "source": [
    "import utils\n",
    "import utils_data as ds\n",
    "# Load dataset\n",
    "dataset_filepaths, dataset_brat_folders = utils.get_valid_dataset_filepaths(parameters)\n",
    "dataset = ds.DatasetP(verbose=False, debug=False)\n",
    "token_to_vector = dataset.load_dataset(dataset_filepaths, parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedded_characters: Tensor(\"character_embedding/embedded_characters:0\", shape=(?, ?, 25), dtype=float32)\n",
      "embedded_tokens: Tensor(\"token_embedding/embedding_lookup:0\", shape=(?, 100), dtype=float32)\n",
      "token_lstm_input: Tensor(\"concatenate_token_and_character_vectors/token_lstm_input:0\", shape=(?, 163), dtype=float32)\n",
      "token_lstm_input_drop: Tensor(\"dropout/token_lstm_input_drop/mul:0\", shape=(?, 163), dtype=float32)\n",
      "token_lstm_input_drop_expanded: Tensor(\"dropout/token_lstm_input_drop_expanded:0\", shape=(1, ?, 163), dtype=float32)\n",
      "unary_scores_expanded: Tensor(\"crf/unary_scores_expanded:0\", shape=(1, ?, 19), dtype=float32)\n",
      "input_label_indices_flat_batch: Tensor(\"crf/input_label_indices_flat_batch:0\", shape=(1, ?), dtype=int32)\n",
      "sequence_lengths: Tensor(\"crf/sequence_lengths:0\", shape=(1,), dtype=int32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\linhpn.VISC\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_impl.py:96: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from BLSTM_CRF_P import BLSTM_CRF\n",
    "# Create model lstm+crf\n",
    "session_conf = tf.ConfigProto(\n",
    "            intra_op_parallelism_threads=parameters['number_of_cpu_threads'],\n",
    "            inter_op_parallelism_threads=parameters['number_of_cpu_threads'],\n",
    "            device_count={'CPU': 2, 'GPU': parameters['number_of_gpus']},\n",
    "            allow_soft_placement=True,\n",
    "            # automatically choose an existing and supported device to run the operations in case the specified one doesn't exist\n",
    "            log_device_placement=False\n",
    "        )\n",
    "sess = tf.Session(config=session_conf)\n",
    "\n",
    "with sess.as_default():\n",
    "    # Create model and initialize or load pretrained model\n",
    "    ### Instantiate the model\n",
    "    model = BLSTM_CRF(dataset=dataset, token_embedding_dimension=parameters['token_embedding_dimension'],\n",
    "                       character_lstm_hidden_state_dimension=parameters['character_lstm_hidden_state_dimension'],\n",
    "                       token_lstm_hidden_state_dimension=parameters['token_lstm_hidden_state_dimension'],\n",
    "                       character_embedding_dimension=parameters['character_embedding_dimension'],\n",
    "                       gradient_clipping_value=parameters['gradient_clipping_value'],\n",
    "                       learning_rate=parameters['learning_rate'],\n",
    "                       freeze_token_embeddings=parameters['freeze_token_embeddings'],\n",
    "                       optimizer=parameters['optimizer'],\n",
    "                       maximum_number_of_epochs=parameters['maximum_number_of_epochs'])\n",
    "\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load token embeddings... done (0.16 seconds)\n",
      "number_of_token_original_case_found: 14618\n",
      "number_of_token_lowercase_found: 11723\n",
      "number_of_token_digits_replaced_with_zeros_found: 119\n",
      "number_of_token_lowercase_and_digits_replaced_with_zeros_found: 16\n",
      "number_of_loaded_word_vectors: 26476\n",
      "dataset.vocabulary_size: 28984\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "model.load_pretrained_token_embeddings(sess, dataset,embedding_filepath=parameters['token_pretrained_embedding_filepath'],\n",
    "                                                       check_lowercase= parameters['check_for_lowercase'],check_digits=parameters['check_for_digits_replaced_with_zeros'],\n",
    "                                                       token_to_vector=token_to_vector)\n",
    "# Initial params_train\n",
    "transition_params_trained = np.random.rand(len(dataset.unique_labels) + 2,len(dataset.unique_labels) + 2)\n",
    "\n",
    "del token_to_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import copy\n",
    "import os\n",
    "import pickle\n",
    "stats_graph_folder, experiment_timestamp = utils.create_stats_graph_folder(parameters)\n",
    "\n",
    "        # Initialize and save execution details\n",
    "start_time = time.time()\n",
    "# results = {}\n",
    "# results['epoch'] = {}\n",
    "# results['execution_details'] = {}\n",
    "# results['execution_details']['train_start'] = start_time\n",
    "# results['execution_details']['time_stamp'] = experiment_timestamp\n",
    "# results['execution_details']['early_stop'] = False\n",
    "# results['execution_details']['keyboard_interrupt'] = False\n",
    "# results['execution_details']['num_epochs'] = 0\n",
    "# results['model_options'] = copy.copy(parameters)\n",
    "\n",
    "model_folder = os.path.join(stats_graph_folder, 'model')\n",
    "utils.create_folder_if_not_exists(model_folder)\n",
    "\n",
    "pickle.dump(dataset, open(os.path.join(model_folder, 'dataset.pickle'), 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting epoch 0\n",
      "Training completed in 0.00 seconds\n",
      "Evaluate model on the train set\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      B-LOC     0.8635    0.8694    0.8664      1041\n",
      "     B-MISC     0.8863    0.6177    0.7280       858\n",
      "      B-ORG     0.8531    0.8905    0.8714      2485\n",
      "      B-PER     0.9651    0.9825    0.9737      4284\n",
      "      E-LOC     0.8582    0.8722    0.8652      1041\n",
      "     E-MISC     0.9106    0.6527    0.7604       858\n",
      "      E-ORG     0.8525    0.8930    0.8722      2485\n",
      "      E-PER     0.9686    0.9862    0.9773      4284\n",
      "      I-LOC     0.9688    0.2672    0.4189       116\n",
      "     I-MISC     0.8963    0.4074    0.5602       297\n",
      "      I-ORG     0.7786    0.8540    0.8146      1219\n",
      "      I-PER     0.9207    0.8566    0.8875       244\n",
      "          O     0.0000    0.0000    0.0000       562\n",
      "      S-LOC     0.9481    0.9585    0.9533      6099\n",
      "     S-MISC     0.8770    0.8841    0.8805      2580\n",
      "      S-ORG     0.9270    0.8772    0.9014      3836\n",
      "      S-PER     0.9295    0.9050    0.9171      2316\n",
      "\n",
      "avg / total     0.9005    0.8889    0.8919     34605\n",
      "\n",
      "Evaluate model on the valid set\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      B-LOC     0.8700    0.8291    0.8490       234\n",
      "     B-MISC     0.8077    0.5720    0.6697       257\n",
      "      B-ORG     0.7773    0.8222    0.7991       450\n",
      "      B-PER     0.9791    0.9854    0.9822      1234\n",
      "      E-LOC     0.8705    0.8333    0.8515       234\n",
      "     E-MISC     0.8385    0.6265    0.7171       257\n",
      "      E-ORG     0.7822    0.8378    0.8090       450\n",
      "      E-PER     0.9767    0.9838    0.9802      1234\n",
      "      I-LOC     1.0000    0.3478    0.5161        23\n",
      "     I-MISC     0.7647    0.2921    0.4228        89\n",
      "      I-ORG     0.7864    0.8073    0.7967       301\n",
      "      I-PER     0.9811    0.7123    0.8254        73\n",
      "          O     0.0000    0.0000    0.0000       126\n",
      "      S-LOC     0.9551    0.9688    0.9619      1603\n",
      "     S-MISC     0.8679    0.8992    0.8833       665\n",
      "      S-ORG     0.9174    0.8855    0.9012       891\n",
      "      S-PER     0.8962    0.9227    0.9092       608\n",
      "\n",
      "avg / total     0.8953    0.8826    0.8861      8729\n",
      "\n",
      "Evaluate model on the test set\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      B-LOC     0.7635    0.7931    0.7780       232\n",
      "     B-MISC     0.6566    0.6158    0.6356       177\n",
      "      B-ORG     0.7527    0.8411    0.7945       579\n",
      "      B-PER     0.9754    0.9862    0.9808      1086\n",
      "      E-LOC     0.7727    0.8060    0.7890       232\n",
      "     E-MISC     0.6864    0.6554    0.6705       177\n",
      "      E-ORG     0.7650    0.8601    0.8098       579\n",
      "      E-PER     0.9709    0.9825    0.9767      1086\n",
      "      I-LOC     0.0000    0.0000    0.0000        25\n",
      "     I-MISC     0.7333    0.5641    0.6377        39\n",
      "      I-ORG     0.7019    0.8555    0.7711       256\n",
      "      I-PER     0.9355    0.8286    0.8788        70\n",
      "          O     0.0000    0.0000    0.0000       346\n",
      "      S-LOC     0.9128    0.9255    0.9191      1436\n",
      "     S-MISC     0.8216    0.8419    0.8316       525\n",
      "      S-ORG     0.9084    0.8521    0.8794      1082\n",
      "      S-PER     0.9093    0.8493    0.8783       531\n",
      "\n",
      "avg / total     0.8357    0.8468    0.8403      8458\n",
      "\n",
      "\n",
      "Starting epoch 1\n",
      "Training completed in 288.42 seconds\n",
      "Evaluate model on the train set\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      B-LOC     0.7782    0.9337    0.8489      1041\n",
      "     B-MISC     0.8941    0.6690    0.7653       858\n",
      "      B-ORG     0.9108    0.8386    0.8732      2485\n",
      "      B-PER     0.9646    0.9853    0.9748      4284\n",
      "      E-LOC     0.7654    0.9308    0.8401      1041\n",
      "     E-MISC     0.9202    0.6993    0.7947       858\n",
      "      E-ORG     0.9102    0.8439    0.8758      2485\n",
      "      E-PER     0.9700    0.9902    0.9800      4284\n",
      "      I-LOC     0.7286    0.4397    0.5484       116\n",
      "     I-MISC     0.8508    0.5185    0.6444       297\n",
      "      I-ORG     0.8803    0.8023    0.8395      1219\n",
      "      I-PER     0.9339    0.8689    0.9002       244\n",
      "          O     0.0000    0.0000    0.0000       336\n",
      "      S-LOC     0.9512    0.9620    0.9566      6099\n",
      "     S-MISC     0.9089    0.8779    0.8931      2580\n",
      "      S-ORG     0.9516    0.8764    0.9125      3836\n",
      "      S-PER     0.9232    0.9236    0.9234      2316\n",
      "\n",
      "avg / total     0.9177    0.8955    0.9044     34379\n",
      "\n",
      "Evaluate model on the valid set\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      B-LOC     0.7526    0.9231    0.8292       234\n",
      "     B-MISC     0.8158    0.6031    0.6935       257\n",
      "      B-ORG     0.8329    0.7644    0.7972       450\n",
      "      B-PER     0.9854    0.9870    0.9862      1234\n",
      "      E-LOC     0.7483    0.9145    0.8231       234\n",
      "     E-MISC     0.8550    0.6654    0.7484       257\n",
      "      E-ORG     0.8469    0.7867    0.8157       450\n",
      "      E-PER     0.9838    0.9870    0.9854      1234\n",
      "      I-LOC     0.8000    0.5217    0.6316        23\n",
      "     I-MISC     0.7391    0.3820    0.5037        89\n",
      "      I-ORG     0.8745    0.7409    0.8022       301\n",
      "      I-PER     0.9623    0.6986    0.8095        73\n",
      "          O     0.0000    0.0000    0.0000       106\n",
      "      S-LOC     0.9431    0.9726    0.9576      1603\n",
      "     S-MISC     0.9023    0.8887    0.8955       665\n",
      "      S-ORG     0.9287    0.8631    0.8947       891\n",
      "      S-PER     0.8958    0.9046    0.9002       608\n",
      "\n",
      "avg / total     0.9035    0.8817    0.8900      8709\n",
      "\n",
      "Evaluate model on the test set\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      B-LOC     0.6209    0.8966    0.7337       232\n",
      "     B-MISC     0.6182    0.5763    0.5965       177\n",
      "      B-ORG     0.8138    0.7547    0.7832       579\n",
      "      B-PER     0.9692    0.9843    0.9767      1086\n",
      "      E-LOC     0.6064    0.8966    0.7235       232\n",
      "     E-MISC     0.6829    0.6328    0.6569       177\n",
      "      E-ORG     0.8327    0.7824    0.8068       579\n",
      "      E-PER     0.9691    0.9816    0.9753      1086\n",
      "      I-LOC     0.4706    0.3200    0.3810        25\n",
      "     I-MISC     0.6000    0.6154    0.6076        39\n",
      "      I-ORG     0.7698    0.7578    0.7638       256\n",
      "      I-PER     0.9375    0.8571    0.8955        70\n",
      "          O     0.0000    0.0000    0.0000       294\n",
      "      S-LOC     0.9139    0.9311    0.9224      1436\n",
      "     S-MISC     0.8580    0.8400    0.8489       525\n",
      "      S-ORG     0.9102    0.8521    0.8802      1082\n",
      "      S-PER     0.9245    0.8305    0.8750       531\n",
      "\n",
      "avg / total     0.8458    0.8425    0.8421      8406\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bad_counter = 0  # number of epochs with no improvement on the validation test in terms of F1-score\n",
    "previous_best_valid_f1_score = 0\n",
    "epoch_number = -1\n",
    "\n",
    "while True:\n",
    "\n",
    "    step = 0\n",
    "    epoch_number += 1\n",
    "    print('\\nStarting epoch {0}'.format(epoch_number))\n",
    "\n",
    "    epoch_start_time = time.time()\n",
    "\n",
    "    if epoch_number != 0:\n",
    "        # Train model: loop over all sequences of training set with shuffling\n",
    "        sequence_numbers = list(range(len(dataset.token_indices['train'])))\n",
    "        random.shuffle(sequence_numbers)\n",
    "        for sequence_number in sequence_numbers:\n",
    "            transition_params_trained = train.train_step(sess, dataset, sequence_number, model, parameters['dropout_rate'])\n",
    "            step += 1\n",
    "            if step % 10 == 0:\n",
    "                print('Training {0:.2f}% done'.format(step / len(sequence_numbers) * 100), end='\\r', flush=True)\n",
    "\n",
    "    epoch_elapsed_training_time = time.time() - epoch_start_time\n",
    "    print('Training completed in {0:.2f} seconds'.format(epoch_elapsed_training_time), flush=True)\n",
    "\n",
    "    y_pred, y_true, output_filepaths = train.predict_labels(sess=sess,model= model,transition_params_trained= transition_params_trained,\n",
    "                                                                         dataset=dataset,epoch_number= epoch_number,\n",
    "                                                                        stats_graph_folder= stats_graph_folder,dataset_filepaths= dataset_filepaths,\n",
    "                                                                        tagging_format= parameters['tagging_format'])\n",
    "\n",
    "       \n",
    "#     if epoch_number % 3 ==0:\n",
    "    model.saver.save(sess, os.path.join(model_folder, 'model_{0:05d}.ckpt'.format(epoch_number%3)))\n",
    "        \n",
    "    if epoch_number > 0 :\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.token_patterns['train'][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.label_vector_indices['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1066, 0, 238, 835, 6, 4019, 229, 8754, 2]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.token_indices['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'EU'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.index_to_token[1066]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prediction_count=0\n",
    "def predict(text):\n",
    "    #         if prediction_count == 1:\n",
    "    parameters['dataset_text_folder'] = os.path.join('..', 'data', 'temp')\n",
    "    stats_graph_folder, _ = utils.create_stats_graph_folder(parameters)\n",
    "\n",
    "    # Update the deploy folder, file, and dataset\n",
    "    dataset_type = 'deploy'\n",
    "    ### Delete all deployment data\n",
    "    for filepath in glob.glob(os.path.join(parameters['dataset_text_folder'], '{0}*'.format(dataset_type))):\n",
    "        if os.path.isdir(filepath):\n",
    "            shutil.rmtree(filepath)\n",
    "        else:\n",
    "            os.remove(filepath)\n",
    "    ### Create brat folder and file\n",
    "    dataset_brat_deploy_folder = os.path.join(parameters['dataset_text_folder'], dataset_type)\n",
    "    utils.create_folder_if_not_exists(dataset_brat_deploy_folder)\n",
    "    dataset_brat_deploy_filepath = os.path.join(dataset_brat_deploy_folder, 'temp_{0}.txt'.format(\n",
    "        str(prediction_count).zfill(5)))  # self._get_dataset_brat_deploy_filepath(dataset_brat_deploy_folder)\n",
    "    with codecs.open(dataset_brat_deploy_filepath, 'w', 'UTF-8') as f:\n",
    "        f.write(text)\n",
    "    ### Update deploy filepaths\n",
    "    dataset_filepaths, dataset_brat_folders = utils.get_valid_dataset_filepaths(parameters,\n",
    "                                                                           dataset_types=[dataset_type])\n",
    "    dataset_filepaths.update(dataset_filepaths)\n",
    "    dataset_brat_folders.update(dataset_brat_folders)\n",
    "    ### Update the dataset for the new deploy set\n",
    "    dataset.update_dataset(dataset_filepaths, [dataset_type])\n",
    "\n",
    "    # Predict labels and output brat\n",
    "    output_filepaths = {}\n",
    "    prediction_output = train.prediction_step(sess, dataset, dataset_type, model,\n",
    "                                              transition_params_trained, stats_graph_folder,\n",
    "                                              prediction_count, dataset_filepaths, parameters['tagging_format'])\n",
    "    predictions , _, output_filepaths[dataset_type] = prediction_output\n",
    "    \n",
    "    print([dataset.index_to_label[prediction] for prediction in predictions])\n",
    "    conll2brat.output_brat(output_filepaths, dataset_brat_folders, stats_graph_folder, overwrite=True)\n",
    "\n",
    "    # Print and output result\n",
    "    text_filepath = os.path.join(stats_graph_folder, 'brat', 'deploy',\n",
    "                                 os.path.basename(dataset_brat_deploy_filepath))\n",
    "    annotation_filepath = os.path.join(stats_graph_folder, 'brat', 'deploy', '{0}.ann'.format(\n",
    "        utils.get_basename_without_extension(dataset_brat_deploy_filepath)))\n",
    "    text2, entities = brat2conll.get_entities_from_brat(text_filepath, annotation_filepath, verbose=True)\n",
    "    assert (text == text2)\n",
    "    return entities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Formatting deploy set from BRAT to CONLL... Done.\n",
      "Converting CONLL from BIO to BIOES format... Done.\n",
      "Predict labels for the deploy set\n",
      "['O', 'O', 'O', 'B-PER', 'E-PER']\n",
      "Formatting 000_deploy set from CONLL to BRAT... Done.\n",
      "\n",
      "text:\n",
      "my name is  Ngoc Linh\n",
      "\n",
      "entity: {'id': 'T1', 'type': 'PER', 'start': 12, 'end': 21, 'text': 'Ngoc Linh'}\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'end': 21, 'id': 'T1', 'start': 12, 'text': 'Ngoc Linh', 'type': 'PER'}]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict('my name is  Ngoc Linh')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
