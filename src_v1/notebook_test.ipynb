{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\linhpn.VISC\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import matplotlib\n",
    "\n",
    "matplotlib.use('Agg')\n",
    "import train\n",
    "import dataset as ds\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.tensorboard.plugins import projector\n",
    "\n",
    "import utils\n",
    "import os\n",
    "import conll2brat\n",
    "import glob\n",
    "import codecs\n",
    "import shutil\n",
    "import time\n",
    "import copy\n",
    "import evaluate\n",
    "import random\n",
    "import pickle\n",
    "import brat2conll\n",
    "import numpy as np\n",
    "import utils_nlp\n",
    "import distutils.util as distutils_util\n",
    "import configparser\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "parameters = {'pretrained_model_folder':'../model',\n",
    "                      'dataset_text_folder':'../../../ML_EntityData/data/en',\n",
    "                      'character_embedding_dimension':25,\n",
    "                      'character_lstm_hidden_state_dimension':25,\n",
    "                      'check_for_digits_replaced_with_zeros':True,\n",
    "                      'check_for_lowercase':True,\n",
    "                      'debug':False,\n",
    "                      'dropout_rate':0.5,\n",
    "                      'experiment_name':'test',\n",
    "                      'freeze_token_embeddings':False,\n",
    "                      'gradient_clipping_value':5.0,\n",
    "                      'learning_rate':0.005,\n",
    "                      'load_only_pretrained_token_embeddings':False,\n",
    "                      'load_all_pretrained_token_embeddings':False,\n",
    "#                       'main_evaluation_mode':'conll',\n",
    "                      'maximum_number_of_epochs':500,\n",
    "                      'number_of_cpu_threads':8,\n",
    "                      'number_of_gpus':0,\n",
    "                      'optimizer':'sgd',\n",
    "                      'output_folder':'../../../ML_EntityData/output',\n",
    "                      'patience':10,\n",
    "#                       'plot_format':'pdf',\n",
    "                      'reload_character_embeddings':True,\n",
    "                      'reload_character_lstm':True,\n",
    "                      'reload_crf':True,\n",
    "                      'reload_feedforward':True,\n",
    "                      'reload_token_embeddings':True,\n",
    "                      'reload_token_lstm':True,\n",
    "                      'remap_unknown_tokens_to_unk':True,\n",
    "#                       'spacylanguage':'en',\n",
    "                      'tagging_format':'bioes',\n",
    "                      'token_embedding_dimension':100,\n",
    "                      'token_lstm_hidden_state_dimension':100,\n",
    "                      'token_pretrained_embedding_filepath':'../../../ML_EntityData/embedding/en/glove.6B.100d.txt',\n",
    "#                       'tokenizer':'spacy',\n",
    "                      'train_model':True,\n",
    "                      'use_character_lstm':True,\n",
    "#                       'use_crf':True,\n",
    "                      'use_pretrained_model':False,\n",
    "                      'verbose':False}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking the validity of BRAT-formatted train set... Done.\n",
      "Checking compatibility between CONLL and BRAT for train_compatible_with_brat set ... Done.\n",
      "Checking validity of CONLL BIOES format... Done.\n",
      "Checking the validity of BRAT-formatted valid set... Done.\n",
      "Checking compatibility between CONLL and BRAT for valid_compatible_with_brat set ... Done.\n",
      "Checking validity of CONLL BIOES format... Done.\n",
      "Checking the validity of BRAT-formatted test set... Done.\n",
      "Checking compatibility between CONLL and BRAT for test_compatible_with_brat set ... Done.\n",
      "Checking validity of CONLL BIOES format... Done.\n",
      "Load dataset... done (46.84 seconds)\n"
     ]
    }
   ],
   "source": [
    "import utils\n",
    "import dataset as ds\n",
    "# Load dataset\n",
    "dataset_filepaths, dataset_brat_folders = utils.get_valid_dataset_filepaths(parameters)\n",
    "dataset = ds.Dataset(verbose=False, debug=False)\n",
    "token_to_vector = dataset.load_dataset(dataset_filepaths, parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedded_characters: Tensor(\"character_embedding/embedded_characters:0\", shape=(?, ?, 25), dtype=float32)\n",
      "embedded_tokens: Tensor(\"token_embedding/embedding_lookup:0\", shape=(?, 100), dtype=float32)\n",
      "token_lstm_input: Tensor(\"concatenate_token_and_character_vectors/token_lstm_input:0\", shape=(?, 150), dtype=float32)\n",
      "token_lstm_input_drop: Tensor(\"dropout/token_lstm_input_drop/mul:0\", shape=(?, 150), dtype=float32)\n",
      "token_lstm_input_drop_expanded: Tensor(\"dropout/token_lstm_input_drop_expanded:0\", shape=(1, ?, 150), dtype=float32)\n",
      "unary_scores_expanded: Tensor(\"crf/unary_scores_expanded:0\", shape=(1, ?, 19), dtype=float32)\n",
      "input_label_indices_flat_batch: Tensor(\"crf/input_label_indices_flat_batch:0\", shape=(1, ?), dtype=int32)\n",
      "sequence_lengths: Tensor(\"crf/sequence_lengths:0\", shape=(1,), dtype=int32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\linhpn.VISC\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_impl.py:96: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from BLSTM_CRF import Char_BLSTM_CRF\n",
    "# Create model lstm+crf\n",
    "session_conf = tf.ConfigProto(\n",
    "            intra_op_parallelism_threads=parameters['number_of_cpu_threads'],\n",
    "            inter_op_parallelism_threads=parameters['number_of_cpu_threads'],\n",
    "            device_count={'CPU': 2, 'GPU': parameters['number_of_gpus']},\n",
    "            allow_soft_placement=True,\n",
    "            # automatically choose an existing and supported device to run the operations in case the specified one doesn't exist\n",
    "            log_device_placement=False\n",
    "        )\n",
    "sess = tf.Session(config=session_conf)\n",
    "\n",
    "with sess.as_default():\n",
    "    # Create model and initialize or load pretrained model\n",
    "    ### Instantiate the model\n",
    "    model = Char_BLSTM_CRF(dataset=dataset, token_embedding_dimension=parameters['token_embedding_dimension'],\n",
    "                       character_lstm_hidden_state_dimension=parameters['character_lstm_hidden_state_dimension'],\n",
    "                       token_lstm_hidden_state_dimension=parameters['token_lstm_hidden_state_dimension'],\n",
    "                       character_embedding_dimension=parameters['character_embedding_dimension'],\n",
    "                       gradient_clipping_value=parameters['gradient_clipping_value'],\n",
    "                       learning_rate=parameters['learning_rate'],\n",
    "                       freeze_token_embeddings=parameters['freeze_token_embeddings'],\n",
    "                       optimizer=parameters['optimizer'],\n",
    "                       maximum_number_of_epochs=parameters['maximum_number_of_epochs'])\n",
    "\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load token embeddings... done (0.25 seconds)\n",
      "number_of_token_original_case_found: 14618\n",
      "number_of_token_lowercase_found: 11723\n",
      "number_of_token_digits_replaced_with_zeros_found: 119\n",
      "number_of_token_lowercase_and_digits_replaced_with_zeros_found: 16\n",
      "number_of_loaded_word_vectors: 26476\n",
      "dataset.vocabulary_size: 28984\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "model.load_pretrained_token_embeddings(sess, dataset,embedding_filepath=parameters['token_pretrained_embedding_filepath'],\n",
    "                                                       check_lowercase= parameters['check_for_lowercase'],check_digits=parameters['check_for_digits_replaced_with_zeros'],\n",
    "                                                       token_to_vector=token_to_vector)\n",
    "# Initial params_train\n",
    "transition_params_trained = np.random.rand(len(dataset.unique_labels) + 2,len(dataset.unique_labels) + 2)\n",
    "\n",
    "del token_to_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import copy\n",
    "import os\n",
    "import pickle\n",
    "stats_graph_folder, experiment_timestamp = utils.create_stats_graph_folder(parameters)\n",
    "\n",
    "        # Initialize and save execution details\n",
    "start_time = time.time()\n",
    "# results = {}\n",
    "# results['epoch'] = {}\n",
    "# results['execution_details'] = {}\n",
    "# results['execution_details']['train_start'] = start_time\n",
    "# results['execution_details']['time_stamp'] = experiment_timestamp\n",
    "# results['execution_details']['early_stop'] = False\n",
    "# results['execution_details']['keyboard_interrupt'] = False\n",
    "# results['execution_details']['num_epochs'] = 0\n",
    "# results['model_options'] = copy.copy(parameters)\n",
    "\n",
    "model_folder = os.path.join(stats_graph_folder, 'model')\n",
    "utils.create_folder_if_not_exists(model_folder)\n",
    "\n",
    "pickle.dump(dataset, open(os.path.join(model_folder, 'dataset.pickle'), 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting epoch 0\n",
      "Training completed in 0.00 seconds\n",
      "Evaluate model on the train set\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      B-LOC     0.9197    0.9241    0.9219      1041\n",
      "     B-MISC     0.8969    0.8520    0.8739       858\n",
      "      B-ORG     0.9253    0.9272    0.9262      2485\n",
      "      B-PER     0.9758    0.9883    0.9820      4284\n",
      "      E-LOC     0.9152    0.9232    0.9192      1041\n",
      "     E-MISC     0.8995    0.8555    0.8769       858\n",
      "      E-ORG     0.9250    0.9280    0.9265      2485\n",
      "      E-PER     0.9788    0.9911    0.9849      4284\n",
      "      I-LOC     0.8846    0.5948    0.7113       116\n",
      "     I-MISC     0.8703    0.7003    0.7761       297\n",
      "      I-ORG     0.8861    0.8999    0.8930      1219\n",
      "      I-PER     0.9200    0.9426    0.9312       244\n",
      "          O     0.0000    0.0000    0.0000       378\n",
      "      S-LOC     0.9545    0.9766    0.9654      6099\n",
      "     S-MISC     0.9638    0.8872    0.9239      2580\n",
      "      S-ORG     0.9646    0.9082    0.9356      3836\n",
      "      S-PER     0.9255    0.9542    0.9396      2316\n",
      "\n",
      "avg / total     0.9366    0.9303    0.9330     34421\n",
      "\n",
      "Evaluate model on the valid set\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      B-LOC     0.8851    0.8889    0.8870       234\n",
      "     B-MISC     0.7918    0.7549    0.7729       257\n",
      "      B-ORG     0.8282    0.8356    0.8319       450\n",
      "      B-PER     0.9870    0.9846    0.9858      1234\n",
      "      E-LOC     0.8692    0.8803    0.8747       234\n",
      "     E-MISC     0.8259    0.7938    0.8095       257\n",
      "      E-ORG     0.8396    0.8489    0.8442       450\n",
      "      E-PER     0.9846    0.9822    0.9834      1234\n",
      "      I-LOC     0.7222    0.5652    0.6341        23\n",
      "     I-MISC     0.7581    0.5281    0.6225        89\n",
      "      I-ORG     0.8791    0.7973    0.8362       301\n",
      "      I-PER     0.9804    0.6849    0.8065        73\n",
      "          O     0.0000    0.0000    0.0000       125\n",
      "      S-LOC     0.9474    0.9775    0.9622      1603\n",
      "     S-MISC     0.9434    0.8767    0.9088       665\n",
      "      S-ORG     0.9371    0.9024    0.9194       891\n",
      "      S-PER     0.8866    0.9391    0.9121       608\n",
      "\n",
      "avg / total     0.9109    0.9019    0.9056      8728\n",
      "\n",
      "Evaluate model on the test set\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      B-LOC     0.7435    0.8621    0.7984       232\n",
      "     B-MISC     0.5902    0.6836    0.6335       177\n",
      "      B-ORG     0.8162    0.8359    0.8259       579\n",
      "      B-PER     0.9772    0.9862    0.9817      1086\n",
      "      E-LOC     0.7370    0.8578    0.7928       232\n",
      "     E-MISC     0.6244    0.7232    0.6702       177\n",
      "      E-ORG     0.8243    0.8428    0.8335       579\n",
      "      E-PER     0.9771    0.9843    0.9807      1086\n",
      "      I-LOC     0.4800    0.4800    0.4800        25\n",
      "     I-MISC     0.6389    0.5897    0.6133        39\n",
      "      I-ORG     0.7544    0.8398    0.7948       256\n",
      "      I-PER     0.9420    0.9286    0.9353        70\n",
      "          O     0.0000    0.0000    0.0000       308\n",
      "      S-LOC     0.9034    0.9380    0.9204      1436\n",
      "     S-MISC     0.8884    0.8190    0.8523       525\n",
      "      S-ORG     0.9250    0.8669    0.8950      1082\n",
      "      S-PER     0.9263    0.8757    0.9003       531\n",
      "\n",
      "avg / total     0.8531    0.8616    0.8566      8420\n",
      "\n",
      "\n",
      "Starting epoch 1\n",
      "Training completed in 278.94 seconds\n",
      "Evaluate model on the train set\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      B-LOC     0.9182    0.9270    0.9226      1041\n",
      "     B-MISC     0.9005    0.8543    0.8768       858\n",
      "      B-ORG     0.9277    0.9292    0.9284      2485\n",
      "      B-PER     0.9801    0.9888    0.9844      4284\n",
      "      E-LOC     0.9163    0.9251    0.9207      1041\n",
      "     E-MISC     0.9115    0.8648    0.8876       858\n",
      "      E-ORG     0.9269    0.9288    0.9278      2485\n",
      "      E-PER     0.9831    0.9923    0.9877      4284\n",
      "      I-LOC     0.8721    0.6466    0.7426       116\n",
      "     I-MISC     0.8507    0.7677    0.8071       297\n",
      "      I-ORG     0.8606    0.9418    0.8993      1219\n",
      "      I-PER     0.9344    0.9344    0.9344       244\n",
      "          O     0.0000    0.0000    0.0000       351\n",
      "      S-LOC     0.9756    0.9707    0.9731      6099\n",
      "     S-MISC     0.9609    0.8946    0.9265      2580\n",
      "      S-ORG     0.9519    0.9283    0.9399      3836\n",
      "      S-PER     0.9607    0.9396    0.9500      2316\n",
      "\n",
      "avg / total     0.9426    0.9348    0.9384     34394\n",
      "\n",
      "Evaluate model on the valid set\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      B-LOC     0.8970    0.8932    0.8951       234\n",
      "     B-MISC     0.7773    0.7743    0.7758       257\n",
      "      B-ORG     0.8462    0.8311    0.8386       450\n",
      "      B-PER     0.9829    0.9781    0.9805      1234\n",
      "      E-LOC     0.8874    0.8761    0.8817       234\n",
      "     E-MISC     0.8031    0.8093    0.8062       257\n",
      "      E-ORG     0.8756    0.8600    0.8677       450\n",
      "      E-PER     0.9837    0.9789    0.9813      1234\n",
      "      I-LOC     0.7778    0.6087    0.6829        23\n",
      "     I-MISC     0.6857    0.5393    0.6038        89\n",
      "      I-ORG     0.8515    0.8571    0.8543       301\n",
      "      I-PER     0.9623    0.6986    0.8095        73\n",
      "          O     0.0000    0.0000    0.0000       118\n",
      "      S-LOC     0.9594    0.9726    0.9659      1603\n",
      "     S-MISC     0.9436    0.8812    0.9114       665\n",
      "      S-ORG     0.9306    0.9181    0.9243       891\n",
      "      S-PER     0.9190    0.9145    0.9167       608\n",
      "\n",
      "avg / total     0.9155    0.9044    0.9095      8721\n",
      "\n",
      "Evaluate model on the test set\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      B-LOC     0.7804    0.8578    0.8172       232\n",
      "     B-MISC     0.6392    0.7006    0.6685       177\n",
      "      B-ORG     0.8231    0.8359    0.8295       579\n",
      "      B-PER     0.9737    0.9899    0.9817      1086\n",
      "      E-LOC     0.7804    0.8578    0.8172       232\n",
      "     E-MISC     0.6564    0.7232    0.6882       177\n",
      "      E-ORG     0.8299    0.8428    0.8363       579\n",
      "      E-PER     0.9719    0.9862    0.9790      1086\n",
      "      I-LOC     0.4615    0.4800    0.4706        25\n",
      "     I-MISC     0.6136    0.6923    0.6506        39\n",
      "      I-ORG     0.6792    0.8438    0.7526       256\n",
      "      I-PER     0.9275    0.9143    0.9209        70\n",
      "          O     0.0000    0.0000    0.0000       295\n",
      "      S-LOC     0.9273    0.9325    0.9299      1436\n",
      "     S-MISC     0.8753    0.8286    0.8513       525\n",
      "      S-ORG     0.9154    0.8799    0.8973      1082\n",
      "      S-PER     0.9467    0.8362    0.8880       531\n",
      "\n",
      "avg / total     0.8588    0.8632    0.8602      8407\n",
      "\n",
      "\n",
      "Starting epoch 2\n",
      "Training completed in 291.86 seconds\n",
      "Evaluate model on the train set\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      B-LOC     0.9236    0.9404    0.9319      1041\n",
      "     B-MISC     0.9272    0.8613    0.8931       858\n",
      "      B-ORG     0.9344    0.9400    0.9372      2485\n",
      "      B-PER     0.9826    0.9865    0.9845      4284\n",
      "      E-LOC     0.9189    0.9356    0.9272      1041\n",
      "     E-MISC     0.9198    0.8555    0.8865       858\n",
      "      E-ORG     0.9333    0.9400    0.9366      2485\n",
      "      E-PER     0.9854    0.9897    0.9875      4284\n",
      "      I-LOC     0.8396    0.7672    0.8018       116\n",
      "     I-MISC     0.9079    0.6970    0.7886       297\n",
      "      I-ORG     0.9174    0.9016    0.9094      1219\n",
      "      I-PER     0.9532    0.9180    0.9353       244\n",
      "          O     0.0000    0.0000    0.0000       402\n",
      "      S-LOC     0.9644    0.9805    0.9724      6099\n",
      "     S-MISC     0.9480    0.9248    0.9362      2580\n",
      "      S-ORG     0.9620    0.9361    0.9489      3836\n",
      "      S-PER     0.9233    0.9668    0.9445      2316\n",
      "\n",
      "avg / total     0.9420    0.9400    0.9408     34445\n",
      "\n",
      "Evaluate model on the valid set\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      B-LOC     0.8983    0.9060    0.9021       234\n",
      "     B-MISC     0.8361    0.7743    0.8040       257\n",
      "      B-ORG     0.8495    0.8778    0.8634       450\n",
      "      B-PER     0.9934    0.9765    0.9849      1234\n",
      "      E-LOC     0.8856    0.8932    0.8894       234\n",
      "     E-MISC     0.8577    0.7977    0.8266       257\n",
      "      E-ORG     0.8584    0.8889    0.8734       450\n",
      "      E-PER     0.9909    0.9757    0.9833      1234\n",
      "      I-LOC     0.7143    0.6522    0.6818        23\n",
      "     I-MISC     0.7778    0.5506    0.6447        89\n",
      "      I-ORG     0.9004    0.8405    0.8694       301\n",
      "      I-PER     0.9800    0.6712    0.7967        73\n",
      "          O     0.0000    0.0000    0.0000       126\n",
      "      S-LOC     0.9554    0.9744    0.9648      1603\n",
      "     S-MISC     0.9172    0.8992    0.9081       665\n",
      "      S-ORG     0.9447    0.9203    0.9323       891\n",
      "      S-PER     0.8695    0.9424    0.9045       608\n",
      "\n",
      "avg / total     0.9176    0.9105    0.9133      8729\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluate model on the test set\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      B-LOC     0.7710    0.8707    0.8178       232\n",
      "     B-MISC     0.6193    0.6893    0.6524       177\n",
      "      B-ORG     0.8223    0.8394    0.8308       579\n",
      "      B-PER     0.9790    0.9862    0.9826      1086\n",
      "      E-LOC     0.7672    0.8664    0.8138       232\n",
      "     E-MISC     0.6480    0.7175    0.6810       177\n",
      "      E-ORG     0.8291    0.8463    0.8376       579\n",
      "      E-PER     0.9799    0.9853    0.9826      1086\n",
      "      I-LOC     0.6000    0.7200    0.6545        25\n",
      "     I-MISC     0.6875    0.5641    0.6197        39\n",
      "      I-ORG     0.7553    0.8320    0.7918       256\n",
      "      I-PER     0.9265    0.9000    0.9130        70\n",
      "          O     0.0000    0.0000    0.0000       325\n",
      "      S-LOC     0.9151    0.9380    0.9264      1436\n",
      "     S-MISC     0.8315    0.8457    0.8385       525\n",
      "      S-ORG     0.9181    0.8808    0.8991      1082\n",
      "      S-PER     0.9178    0.8832    0.9002       531\n",
      "\n",
      "avg / total     0.8529    0.8650    0.8585      8437\n",
      "\n",
      "\n",
      "Starting epoch 3\n",
      "Training completed in 290.00 seconds\n",
      "Evaluate model on the train set\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      B-LOC     0.9481    0.9308    0.9394      1041\n",
      "     B-MISC     0.9292    0.8869    0.9076       858\n",
      "      B-ORG     0.9439    0.9485    0.9462      2485\n",
      "      B-PER     0.9826    0.9914    0.9870      4284\n",
      "      E-LOC     0.9461    0.9270    0.9364      1041\n",
      "     E-MISC     0.9294    0.8893    0.9089       858\n",
      "      E-ORG     0.9427    0.9473    0.9450      2485\n",
      "      E-PER     0.9852    0.9932    0.9892      4284\n",
      "      I-LOC     0.8571    0.7241    0.7850       116\n",
      "     I-MISC     0.8727    0.7845    0.8262       297\n",
      "      I-ORG     0.9231    0.9253    0.9242      1219\n",
      "      I-PER     0.9317    0.9508    0.9412       244\n",
      "          O     0.0000    0.0000    0.0000       366\n",
      "      S-LOC     0.9766    0.9779    0.9772      6099\n",
      "     S-MISC     0.9765    0.9000    0.9367      2580\n",
      "      S-ORG     0.9244    0.9596    0.9417      3836\n",
      "      S-PER     0.9417    0.9620    0.9517      2316\n",
      "\n",
      "avg / total     0.9474    0.9458    0.9463     34409\n",
      "\n",
      "Evaluate model on the valid set\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      B-LOC     0.9009    0.8932    0.8970       234\n",
      "     B-MISC     0.8145    0.7860    0.8000       257\n",
      "      B-ORG     0.8687    0.8378    0.8529       450\n",
      "      B-PER     0.9846    0.9862    0.9854      1234\n",
      "      E-LOC     0.8961    0.8846    0.8903       234\n",
      "     E-MISC     0.8387    0.8093    0.8238       257\n",
      "      E-ORG     0.8897    0.8600    0.8746       450\n",
      "      E-PER     0.9830    0.9846    0.9838      1234\n",
      "      I-LOC     0.7222    0.5652    0.6341        23\n",
      "     I-MISC     0.7143    0.5618    0.6289        89\n",
      "      I-ORG     0.9000    0.8372    0.8675       301\n",
      "      I-PER     0.9444    0.6986    0.8031        73\n",
      "          O     0.0000    0.0000    0.0000       131\n",
      "      S-LOC     0.9593    0.9713    0.9653      1603\n",
      "     S-MISC     0.9544    0.8812    0.9163       665\n",
      "      S-ORG     0.8902    0.9461    0.9173       891\n",
      "      S-PER     0.8925    0.9424    0.9168       608\n",
      "\n",
      "avg / total     0.9152    0.9099    0.9119      8734\n",
      "\n",
      "Evaluate model on the test set\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      B-LOC     0.7704    0.8534    0.8098       232\n",
      "     B-MISC     0.5707    0.6610    0.6126       177\n",
      "      B-ORG     0.8471    0.8325    0.8397       579\n",
      "      B-PER     0.9755    0.9908    0.9831      1086\n",
      "      E-LOC     0.7665    0.8491    0.8057       232\n",
      "     E-MISC     0.6098    0.7062    0.6545       177\n",
      "      E-ORG     0.8489    0.8342    0.8415       579\n",
      "      E-PER     0.9764    0.9908    0.9835      1086\n",
      "      I-LOC     0.4839    0.6000    0.5357        25\n",
      "     I-MISC     0.5435    0.6410    0.5882        39\n",
      "      I-ORG     0.7801    0.8594    0.8178       256\n",
      "      I-PER     0.9315    0.9714    0.9510        70\n",
      "          O     0.0000    0.0000    0.0000       342\n",
      "      S-LOC     0.9266    0.9325    0.9295      1436\n",
      "     S-MISC     0.8730    0.8114    0.8411       525\n",
      "      S-ORG     0.8555    0.9085    0.8812      1082\n",
      "      S-PER     0.9356    0.8757    0.9047       531\n",
      "\n",
      "avg / total     0.8489    0.8629    0.8553      8454\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "bad_counter = 0  # number of epochs with no improvement on the validation test in terms of F1-score\n",
    "previous_best_valid_f1_score = 0\n",
    "epoch_number = -1\n",
    "\n",
    "while True:\n",
    "\n",
    "    step = 0\n",
    "    epoch_number += 1\n",
    "    print('\\nStarting epoch {0}'.format(epoch_number))\n",
    "\n",
    "    epoch_start_time = time.time()\n",
    "\n",
    "    if epoch_number != 0:\n",
    "        # Train model: loop over all sequences of training set with shuffling\n",
    "        sequence_numbers = list(range(len(dataset.token_indices['train'])))\n",
    "        random.shuffle(sequence_numbers)\n",
    "        for sequence_number in sequence_numbers:\n",
    "            transition_params_trained = train.train_step(sess, dataset, sequence_number, model, parameters['dropout_rate'])\n",
    "            step += 1\n",
    "            if step % 10 == 0:\n",
    "                print('Training {0:.2f}% done'.format(step / len(sequence_numbers) * 100), end='\\r', flush=True)\n",
    "\n",
    "    epoch_elapsed_training_time = time.time() - epoch_start_time\n",
    "    print('Training completed in {0:.2f} seconds'.format(epoch_elapsed_training_time), flush=True)\n",
    "\n",
    "    y_pred, y_true, output_filepaths = train.predict_labels(sess=sess,model= model,transition_params_trained= transition_params_trained,\n",
    "                                                                         dataset=dataset,epoch_number= epoch_number,\n",
    "                                                                        stats_graph_folder= stats_graph_folder,dataset_filepaths= dataset_filepaths,\n",
    "                                                                        tagging_format= parameters['tagging_format'])\n",
    "\n",
    "       \n",
    "#     if epoch_number % 3 ==0:\n",
    "    model.saver.save(sess, os.path.join(model_folder, 'model_{0:05d}.ckpt'.format(epoch_number%3)))\n",
    "        \n",
    "    if epoch_number > 2 :\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "prediction_count=0\n",
    "\n",
    "\n",
    "def predict(text):\n",
    "    #         if prediction_count == 1:\n",
    "    parameters['dataset_text_folder'] = os.path.join('..', 'data', 'temp')\n",
    "    stats_graph_folder, _ = utils.create_stats_graph_folder(parameters)\n",
    "\n",
    "    # Update the deploy folder, file, and dataset\n",
    "    dataset_type = 'deploy'\n",
    "    ### Delete all deployment data\n",
    "    for filepath in glob.glob(os.path.join(parameters['dataset_text_folder'], '{0}*'.format(dataset_type))):\n",
    "        if os.path.isdir(filepath):\n",
    "            shutil.rmtree(filepath)\n",
    "        else:\n",
    "            os.remove(filepath)\n",
    "    ### Create brat folder and file\n",
    "    dataset_brat_deploy_folder = os.path.join(parameters['dataset_text_folder'], dataset_type)\n",
    "    utils.create_folder_if_not_exists(dataset_brat_deploy_folder)\n",
    "    dataset_brat_deploy_filepath = os.path.join(dataset_brat_deploy_folder, 'temp_{0}.txt'.format(\n",
    "        str(prediction_count).zfill(5)))  # self._get_dataset_brat_deploy_filepath(dataset_brat_deploy_folder)\n",
    "    with codecs.open(dataset_brat_deploy_filepath, 'w', 'UTF-8') as f:\n",
    "        f.write(text)\n",
    "    ### Update deploy filepaths\n",
    "    dataset_filepaths, dataset_brat_folders = utils.get_valid_dataset_filepaths(parameters,\n",
    "                                                                           dataset_types=[dataset_type])\n",
    "    dataset_filepaths.update(dataset_filepaths)\n",
    "    dataset_brat_folders.update(dataset_brat_folders)\n",
    "    ### Update the dataset for the new deploy set\n",
    "    dataset.update_dataset(dataset_filepaths, [dataset_type])\n",
    "\n",
    "    # Predict labels and output brat\n",
    "    output_filepaths = {}\n",
    "    prediction_output = train.prediction_step(sess, dataset, dataset_type, model,\n",
    "                                              transition_params_trained, stats_graph_folder,\n",
    "                                              prediction_count, dataset_filepaths, parameters['tagging_format'])\n",
    "    predictions , _, output_filepaths[dataset_type] = prediction_output\n",
    "    \n",
    "    print([dataset.index_to_label[prediction] for prediction in predictions])\n",
    "    conll2brat.output_brat(output_filepaths, dataset_brat_folders, stats_graph_folder, overwrite=True)\n",
    "\n",
    "    # Print and output result\n",
    "    text_filepath = os.path.join(stats_graph_folder, 'brat', 'deploy',\n",
    "                                 os.path.basename(dataset_brat_deploy_filepath))\n",
    "    annotation_filepath = os.path.join(stats_graph_folder, 'brat', 'deploy', '{0}.ann'.format(\n",
    "        utils.get_basename_without_extension(dataset_brat_deploy_filepath)))\n",
    "    text2, entities = brat2conll.get_entities_from_brat(text_filepath, annotation_filepath, verbose=True)\n",
    "    assert (text == text2)\n",
    "    return entities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Formatting deploy set from BRAT to CONLL... Done.\n",
      "Converting CONLL from BIO to BIOES format... Done.\n",
      "Predict labels for the deploy set\n",
      "['O', 'O', 'O', 'B-ORG', 'I-ORG', 'E-ORG']\n",
      "Formatting 000_deploy set from CONLL to BRAT... Done.\n",
      "\n",
      "text:\n",
      "my name is Pham  Ngoc Linh\n",
      "\n",
      "entity: {'id': 'T1', 'type': 'ORG', 'start': 11, 'end': 26, 'text': 'Pham Ngoc Linh'}\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'end': 26, 'id': 'T1', 'start': 11, 'text': 'Pham Ngoc Linh', 'type': 'ORG'}]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict('my name is Pham  Ngoc Linh')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
